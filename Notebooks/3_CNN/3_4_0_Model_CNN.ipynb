{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network with own Embedding and fined tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travail effectué\n",
    "* CNN avec XX couches\n",
    "* Avec fined tuning\n",
    "\n",
    "### Conclusion\n",
    "* xxx\n",
    "* xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../../data_models/df_train_undersampling.csv\")\n",
    "df_val = pd.read_csv(\"../../data_models/df_val.csv\")\n",
    "df_test = pd.read_csv(\"../../data_models/df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = df_train[\"text_clean\"].values.astype(str)\n",
    "sentences_val = df_val[\"text_clean\"].values.astype(str)\n",
    "sentences_test = df_test[\"text_clean\"].values.astype(str)\n",
    "\n",
    "y_train = df_train[\"Insult\"].values\n",
    "y_val = df_val[\"Insult\"].values\n",
    "y_test = df_test[\"Insult\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_val = tokenizer.texts_to_sequences(sentences_val)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexing is ordered after the most common words in the text, which you can see by the word the having the index 1. It is important to note that the index 0 is reserved and is not assigned to any word. This zero index is used for padding, which I’ll introduce in a moment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem that we have is that each text sequence has in most cases different length of words. To counter this, you can use pad_sequence() which simply pads the sequence of words with zeros. By default, it prepends zeros but we want to append them. Typically it does not matter whether you prepend or append zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(num_filters=[32, 64, 128],\n",
    "                  kernel_size=[3, 5, 7, 9],\n",
    "                  vocab_size=[vocab_size], \n",
    "                  embedding_dim=[30,50,70],\n",
    "                  maxlen=[maxlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=epochs, batch_size=10, verbose=False)\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=4, verbose=1, n_iter=4)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate testing set\n",
    "val_accuracy = grid.score(X_val, y_val)\n",
    "test_accuracy = grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score :  0.6311475336551666\n",
      "Best params {'vocab_size': 6326, 'num_filters': 128, 'maxlen': 100, 'kernel_size': 9, 'embedding_dim': 50}\n",
      "Validation Accuracy :  0.7772152\n",
      "Test Accuracy :  0.76841706\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score : \",grid_result.best_score_)\n",
    "print(\"Best params\", grid_result.best_params_)\n",
    "print(\"Validation Accuracy : \",val_accuracy)\n",
    "print(\"Test Accuracy : \",test_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultat recherche d'embedding dimension\n",
    "* Best score :  0.6311475336551666\n",
    "* Best params {'vocab_size': 6326, 'num_filters': 128, 'maxlen': 100, 'kernel_size': 9, 'embedding_dim': 50}\n",
    "* Validation Accuracy :  0.7772152\n",
    "* Test Accuracy :  0.76841706"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
